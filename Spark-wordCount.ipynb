{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ad15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5d2fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"PythonWordCount\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81871539",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileRdd = sc.textFile(\"change.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899adb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2656\n"
     ]
    }
   ],
   "source": [
    "FileLen = fileRdd.map(lambda s:len(s)).reduce(lambda a,b:a+b)\n",
    "print(FileLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0087a26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Notepad++': 2, 'v8': 1, 'new': 6, 'features,': 1, 'enhancements': 1, '&': 3, 'bug-fixes:': 1, '': 20, '1.': 2, 'Add': 15, 'Dark': 1, 'Mode.': 1, '2.': 2, 'ARM64': 1, 'build': 3, 'is': 3, 'available.': 1, '3.': 2, 'One': 1, 'button': 3, 'to': 10, 'them': 1, 'all': 2, '-': 1, 'with': 2, 'Scintilla': 1, 'static': 1, 'lib': 1, 'and': 3, 'Boost': 1, 'RegExpr': 1, 'headers.': 1, '4.': 1, 'Fluent': 1, 'UI': 1, 'icons': 1, 'for': 6, 'toolbar.': 1, '5.': 1, 'feature': 2, '\"Distraction': 1, 'Free': 2, 'Mode\"': 1, '(Menu': 1, '\"View->Distraction': 1, 'Mode\").': 1, '6.': 1, 'API': 1, 'NPPM_ADDTOOLBARICON_FORDARKMODE': 1, 'dark': 1, 'mode.': 1, '7.': 1, '1': 1, 'click': 1, 'action': 1, '(SHIFT-click)': 1, 'on': 2, 'close': 2, 'tabs': 1, 'in': 15, 'dockable': 1, 'panel.': 1, '8.': 1, 'ability': 5, 'of': 3, 'changing': 1, 'select': 1, 'text': 1, 'foreground': 1, 'color': 1, '(optional).': 2, '9.': 1, 'Allow': 1, 'Replace': 1, 'stop': 1, 'after': 2, 'replacement': 1, '10.': 1, 'Fix': 15, 'append': 1, 'extension': 1, 'not': 4, 'working': 2, 'save': 1, 'dialog.': 2, '11.': 1, 'reverse': 1, 'line': 2, 'order.': 1, '12.': 1, 'style': 1, 'only': 1, 'current': 1, 'instance': 1, 'text.': 1, '13.': 1, 'duplicated': 1, 'entries': 1, 'Auto-Completion': 2, 'pop-up': 1, 'window.': 1, '14.': 1, 'Python': 1, 'Function': 1, 'List': 2, 'showing': 1, 'functions': 1, 'some': 1, 'circumstance.': 1, '15.': 1, 'Enhance': 1, 'Folder': 1, 'as': 2, 'Workspace': 1, 'performance': 2, 'while': 1, 'adding/removing': 1, 'files': 2, 'bulk.': 1, '16.': 1, 'Ada,': 1, 'Fortran,': 1, 'Fortran77': 1, 'Haskell': 1, 'function': 2, 'lists.': 1, '17.': 1, 'Improve': 2, '\"Open': 1, 'all\"': 1, 'command': 6, 'Search': 4, 'results.': 2, '18.': 1, '\"Copy': 1, 'Pathnames\"': 1, 'results': 1, 'context': 1, 'menu.': 1, '19.': 1, 'Catch': 1, 'regex': 1, 'search': 3, 'exceptions': 1, 'show': 1, 'exception': 1, 'message.': 1, '20.': 1, 'MarkAll': 1, 'Preference': 1, 'settings': 1, 'case': 3, 'word.': 1, '21.': 1, 'regression:': 1, 'Handle': 1, '\"Default': 1, 'Directory\"': 1, 'setting': 1, 'correctly': 1, 'Open/Save': 1, 'File': 1, 'Dialog.': 1, '22.': 1, 'a': 1, 'special': 1, 'character': 2, 'UTF16': 1, 'file': 3, 'crash': 1, 'issue': 2, '23.': 1, '\"Append': 1, 'extension\"': 1, 'checkbox': 1, 'Save': 1, 'As': 1, '24.': 1, 'Copy': 1, 'result': 1, 'available': 1, \"there's\": 1, 'no': 1, 'selection.': 1, '25.': 1, 'padding': 1, 'the': 2, 'edit': 1, 'zone.': 1, '26.': 1, 'Make': 1, 'tab': 2, 'name': 2, 'translatable.': 1, '27.': 1, 'handling': 1, 'RegEx.': 1, '28.': 1, 'dragged': 1, 'out': 1, 'UDL': 2, 'applied': 1, 'instance.': 1, '29.': 1, 'parameter': 1, 'adding': 1, 'specified': 1, 'string': 1, 'app': 1, 'title': 1, 'bar.': 1, '30.': 1, 'ignoring': 1, 'issue.': 4, '31.': 1, '\"Match': 1, 'Whole': 1, 'Word\"': 1, 'option': 1, 'being': 1, 'enabled': 1, 'RegEx': 1, 'Search.': 1, '32.': 1, 'sort': 1, 'column': 1, 'key': 1, 'selection': 1, 'that': 1, 'appears': 1, 'characters.': 1, '33.': 1, '\"Reload': 1, 'Workspace\"': 1, 'menu': 1, '34.': 1, 'view': 1, 'Edge': 1, 'browser': 1, 'failure': 1, 'if': 1, 'contains': 1, 'spaces.': 1, '35.': 1, 'avoid': 1, 'accumulating': 1, 'multiple': 1, '36.': 1, 'UTF-16': 1, 'decoding/encoding': 1, 'code': 1, 'points': 1, 'above': 1, 'U+FFFF.': 1, '37.': 1, '\"Save': 1, 'As\"': 1, 'removing': 1, 'original': 1, 'path': 1, 'from': 2, 'recent': 1, 'history': 1, '38.': 1, 'tool': 1, 'tips': 1, 'mixed': 1, 'up': 1, '39.': 1, 'Remove': 1, 'Microsoft': 1, 'Bing': 1, 'engines': 1, 'Internet': 1, 'command.': 1, 'More': 1, 'fixes': 1, 'implementations': 1, 'detail:': 1, 'https://notepad-plus-plus.org/downloads/v8/': 1, 'Included': 1, 'plugins:': 1, 'NppExport': 1, 'v0.3': 1, 'Converter': 1, 'v4.3': 1, 'Mime': 1, 'Tool': 1, 'v2.6': 1, 'Updater': 1, '(Installer': 1, 'only):': 1, '*': 1, 'WinGup': 1, '(for': 1, 'Notepad++)': 1, 'v5.2': 1}\n"
     ]
    }
   ],
   "source": [
    "fileWordCount = fileRdd.flatMap(lambda x: x.split(' ')) \\\n",
    "                    .map(lambda x:(x,1)).reduceByKeyLocally(lambda a,b:a+b)\n",
    "print(fileWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08ad32a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsSequenceFile.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:547)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:587)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:586)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:586)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:705)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:354)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.api.python.PythonRDD$.saveAsHadoopFile(PythonRDD.scala:616)\r\n\tat org.apache.spark.api.python.PythonRDD$.saveAsSequenceFile(PythonRDD.scala:585)\r\n\tat org.apache.spark.api.python.PythonRDD.saveAsSequenceFile(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:326)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 21 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11316/1768029066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsSequenceFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"testSeqFIle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequenceFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path/to/file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\91807\\desktop\\learnings\\pyspark-learning\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msaveAsSequenceFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         \"\"\"\n\u001b[0;32m   1744\u001b[0m         \u001b[0mpickledRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pickled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m         self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,\n\u001b[0m\u001b[0;32m   1746\u001b[0m                                                    path, compressionCodecClass)\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\91807\\desktop\\learnings\\pyspark-learning\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\91807\\desktop\\learnings\\pyspark-learning\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsSequenceFile.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:547)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:587)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:586)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:586)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:705)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:354)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.api.python.PythonRDD$.saveAsHadoopFile(PythonRDD.scala:616)\r\n\tat org.apache.spark.api.python.PythonRDD$.saveAsSequenceFile(PythonRDD.scala:585)\r\n\tat org.apache.spark.api.python.PythonRDD.saveAsSequenceFile(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:326)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 21 more\r\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
    "rdd.saveAsSequenceFile(\"testSeqFIle\")\n",
    "sorted(sc.sequenceFile(\"path/to/file\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5bacd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
